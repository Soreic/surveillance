<<echo=FALSE>>=

########################################################################
#
# g(theta0_t) = beta0 + sum_{s=1}^S [delta_s*sin(omega_s*t)+gamma_s*cos(omega_s*t)]
# with omega_s = 2*pi*s/period , t=1,...,n
#
# Params:
#  n -
#  S - number of Fourier frequencies
#  coefs=c(beta0,gamma1,delta1,...,gammaS,deltaS) - vector of length 1+2*S
#       beta0 must be given !
#  period=52 - weekly data
#  link - link function g, "log" or "logit"
#
# Returns:
#  vector theta0_t of length n
#######################################################################
simTheta0t <- function(n, coefs, S=0, period=52, link=c("log","logit"),digits=3){
  link <- match.arg(link,c("log","logit"))
  time <- 1:n

  # check dimension of coefs
  if(length(coefs)< 1+2*S)
    stop("coefs too short\n")
  #extract coefs
  beta0 <- coefs[1]
  #beta1 <- coefs[2]

  gammaDelta <- coefs[-1]

  season <- 0

  if(S>0){
    model <- "~-1"
    for(i in 1:S){
      model <- paste(model,"+sin(",2*i,"*pi*time/",period,")+cos(",2*i,"*pi*time/",period,")",sep="")
    }

    X <- model.frame(formula=as.formula(model),data=data.frame("time"=1:n))
    season <- as.matrix(X)%*%gammaDelta
  }
  eta <- rep(beta0,n)+ season   #+beta1*time
  res <- switch(link,
                "log"= round(as.vector(exp(eta)),digits),
                "logit" = round(as.vector(1/(1+exp(-eta))),digits)
                )
  return(res)
}

########################################################################
#
# simulate X1,X2,...,Xn ~ distr(theta_t), t=1,...,n
# (n = length(theta_t) )
#
# Params:
#  theta0t - in-control parameter
#  s - change to detect
#  changepoint - theta_t = theta1_t for t= changepoint,changepoint+1,...,n
#  reps - number of process realisations
#  distr - "poisson" or "binomial"
#  ... - further arguments for distribution
#
# Returns:
#  X - matrix of dimensions size x reps
#######################################################################
simX <- function(theta0t, s=1,changepoint=length(theta0t)+1,reps=1000,distr=c("poisson","binomial"),...){
  distr <- match.arg(distr,c("poisson","binomial"))

  ###############
  # draw binomial random numbers with fixed sample size
  rbinomial <- function(x,p,n=1){
    rbinom(x,size=n,prob=p)
  }
  ################
  distribution <- switch(distr,
                               "poisson" = rpois,
                               "binomial" = rbinomial
                        )
  size <- length(theta0t)

  theta1t <- getTheta1(theta0t,s=s,distr=distr)

  if(changepoint<=1)
    thetat <- theta1t
  else
    thetat <- c(theta0t[1:(changepoint-1)],theta1t[-(1:(changepoint-1))])
    #thetat <- c(theta0t[1:(changepoint)],theta1t[-(1:changepoint)])

  #print(nt)
  X<- matrix(distribution(size*reps,thetat,...),ncol=reps,byrow=FALSE)
  return(X)
}


########################################################################
# simRL
# calculates the run-length for simulated process realisations and
# returns the run length, or alternatively the conditional expected delay (CED)
#
# Params:
#  X - matrix with
#  theta0t - in-control parameter
#  control - list with
#    for "cusumRogerson"
#    s -  change to detect
#    ARL0 - desired average run length
#    hVals - matrix with decision limits for theta_{0,t}
#    rel.tol -
#  cusum - "cusumRogerson" or "cusumRossi"
#  nu -
#  ... - further arguments for cusum
#
# Returns:  list with elements
#  CED - if nu=0 CED corresponds to ARL_1 or ARL_0 (depending on X)
#       if nu >0 CED corresponds to CED(nu) (i.e. ARL_1=CED(0)=CED(1)+1 )
#  fsd - probability of a false sequence detection,
#        i.e. #sequences with alarm/#sequences
#  se  - Monte Carlo estimate of se(ARL)
#  sumRL2, sumRL  -
#  n  - number of sequences used for the computation of CED(nu);
#       only sequences where the first alarm occurs after the change time nu are used
#      (for the computation of ARL_0/ARL_1 no sequences are discarded)
# rl.values - if TRUE the realisations of the run length are retured,
#             if FALSE CED is returned
#######################################################################
simRL <- function(X,theta0t,control,cusum=c("cusumRogerson","cusumRossi"),nu=0,rl.values=FALSE,...){

  cusum <-  match.fun(match.arg(cusum,c("cusumRogerson","cusumRossi")))

  # Detect the first alarm instance in each time series
  # detection delay CED(t)=E[N-\nu | N \geq \nu, \nu=t]
  runmax <- function(x,change=nu) {
    #wm -1 as alarm/cusum contain "timepoint 0"
    wm <- which.max(c(x,1))-1
    # compute alarm delay
    res <- wm - change
    if(res>=0)
      return(res)
    else   #only use sequences with no false alarms before the change occured for computation of CED
      return(NA)
  }

  res <- cusum(x=X, theta0t=theta0t, control=control,...)

  # run lengths before first alarm for each realisation
  rl <- apply(res$alarm,MARGIN=2,runmax,change=nu) #,size=nrow(X))

  #number of sequences actually used to compute the CED
  nSeqCED <- sum(!is.na(rl))
  cat("number of sequences used for computation of CED:",nSeqCED,"\n")

  # at least 1 alarm per realisation ? (run length not truncated)
  alarm <- colSums(res$alarm)>0

  # probability of a false sequence detection = #sequences with alarm/#sequences
  falseSeqDetection <- mean(alarm[!is.na(rl)])
  fsd.se<-sqrt(var(alarm[!is.na(rl)])/length(alarm[!is.na(rl)]))
  #cat("Var fsd:",sqrt(var(alarm[!is.na(rl)])/length(alarm[!is.na(rl)])),"\n")
  rl <- rl[!is.na(rl)]

  # ARL
  ARL <- mean(rl)  #,na.rm=TRUE
  se <- sqrt(var(rl)/length(rl))
  sumRL2 <- sum(rl^2)
  sumRL <- sum(rl)

  cat("ARL:",ARL,"false sequence detection:",falseSeqDetection,"\n")
  #return(list(runLengths=rl,alarm=alarm))
  if(rl.values)
    res <- list(rl=rl,fsd=falseSeqDetection,n=nSeqCED)
  else                                                                               #
    res <- c(CED=ARL,fsd=falseSeqDetection,se=se,sumRL2=sumRL2,sumRL=sumRL,n=nSeqCED,fsd.se=fsd.se)
  return(res)
}

#######################################
# MC estimate of standard error of \hat{CED(nu)}
##############################
MCse <- function(sumX2,sumX,n){
  sqrt(1/(n-1)*(sum(sumX2)/n-(sum(sumX)/n)^2))
}


#################################################################
# Poisson CUSUM (for 1 region) as in Rogerson & Yamada (2004)
# "Approaches to Syndromic Surveillance When Data Consist of Small
#  Regional Counts"
# Morbidity and Mortality Weekly Report 53 79--85
#
# given: in-control mean lambda0t,
#        desired ARL_0,
#        out-of-control mean lambda1t <- lambda0t + s*sqrt(lambda0t)
#        (to detect a s*standard deviation change in lambda0t)
#
# 1) for given lambda0 (e.g. mean(lambda0t)) and respective lambda1 choose
#    k = (lambda1-lambda0)/(ln(lambda1)-ln(lambda0))
# 2) for given k and ARL_0 choose h
# 3) choose k_t based upon lambda0_t and lambda1_t
# 4) for given k_t and ARL_0 choose h_t
# 5) c_t = h/h_t
# 6) build CUSUM S_t = max(0, S_t-1 + c_t*(X_t - k_t))
# 7) give alarm if S_t >= h
#
# Params:
#  x - data (as matrix with "one process" as column)
#  lambda0 - "overall" in-control parameter
#  lambda0t - in-control parameter
#  control - list with
#     ARL0 - desired average run length
#     s -  change to detect
#     distribution - Poisson or binomial
#     hValues - matrix with decision limits for lambda0_t
#     rel.tol - relative tolerance in findH
#  digits - used in findK (should be the same as in control$hValues)
#
# Returns:
#  cusum - vector with cumulative sum
#  alarm - 1 if cusum >= h
#  h,k,lambda0 - CUSUM parameter
####################################################################
cusumRogerson <- function(x,theta0t,theta0=mean(theta0t),control,digits=1,...){
  x <- as.matrix(x)
  #size = length of process
  size <- nrow(x)
  reps <- ncol(x)

  #time-varying size n for Binomial
  nt <- list(...)$nt
  if(is.null(nt))
    nt <- rep(control$n,size)

  #ensure theta0 is not zero
  theta0 <-pmax(10^(-4),theta0)

  theta1 <- getTheta1(theta0,s=control$s,distr=control$distribution)
  theta1t <- getTheta1(theta0t,s=control$s,distr=control$distribution)

  hk <- getHK(theta0,hValues=control$hValues)
  k <- hk[2]
  h <- hk[1]
  cat("k =",k,"h =",h,"\t")

  # initialize the necessary vectors
  # start with cusum[1] = 0
  cusum <- matrix(0,nrow=(size+1), ncol=reps)
  #alarm <- matrix(data = 0, nrow = (size+1), ncol = reps)

  #CUSUM as in Rogerson (2004)
  for(t in 1:size){
    #choose k_t based upon theta_0t and theta_1t
    hkt <- getHK(theta0t[t],hValues=control$hValues)
    kt <- hkt[2]
    kt <- findK(theta0t[t],theta1t[t],distr=control$distribution,roundK=TRUE,digits=digits,
                n=nt[t])

    #for given k_t (theta0t) and ARL_0 choose h_t
    #ht <- getH(lambda0t[t],control$hValues)
    ht <- hkt[1]

    ct <- h/ht
    #ct <- 1
    if(t==1) cat("kt =",kt,"ht =",ht,"\n")

    # compute cumulated sums of observations x corrected with the
    # reference value kt, scaled by factor ct
    cusum[t+1,]<- pmax(0, cusum[t,] + ct*(x[t,]-kt))
    # give alarm if the cusum is larger than h
    #alarm[t+1,] <- round(cusum[t+1,],3) > h
  }
  # give alarm if the cusum is larger than h
  cusum <- round(cusum,3)
  alarm <- cusum >= h
  return(list(cusum=cusum,alarm=alarm,h=h,k=k,theta0=theta0))
}


#----------------------------------------------------------------
# matrix version of algo.cusum (only for Poisson)
#
cusumRossi <- function(x, theta0t=NULL,control = list(range=range, k=0.6, h=3.8,  trans="rossi")){

  # Set the default values if not yet set
  if(is.null(control$k))
    control$k <- 0.6
  if(is.null(control$h))
    control$h <- 3.8

  if(is.null(control$trans))
    control$trans <- "rossi"

  #observed <- disProgObj$observed
  observed <- as.matrix(x)
  timePoint <- control$range[1]

  # Estimate m (the expected number of cases), i.e. parameter lambda of a
  # poisson distribution based on time points 1:t-1
  if(is.null(theta0t)) {
    m <- mean(observed[1:(timePoint-1)])
  } else {
    m <- theta0t
  }

  x <- as.matrix(observed[control$range,])
  standObs <- switch(control$trans,
    # compute standardized variables z3 (proposed by Rossi)
                     "rossi" =    (x - 3*m + 2*sqrt(x*m))/(2*sqrt(m)),
    # compute standardized variables z1 (pearson residuals)
                     "pearson" = (x - m)/sqrt(m),
    # anscombe residuals
                     "anscombe" =  3/2*(x^(2/3)-m^(2/3))/m^(1/6),

    # anscombe residuals as in pierce schafer based on 2nd order approx of E(X)
                     "anscombe2nd" =  (x^(2/3)-(m^(2/3)-m^(-1/3)/9))/(2/3*m^(1/6)),
    # deviance residual for X ~ Po(theta) with m=E(X)=h(\eta)
                     "deviance" = sign(x - m)*(2*(dpois(x,x,log=TRUE)-dpois(x,m,log=TRUE)))^0.5
                     #sign(x - m)*(2*(ifelse(x==0,0,x*(log(x)-log(m)))-(x-m)))^0.5
                     )

  # initialize the necessary vectors
  # start with cusum[timePoint -1] = 0, i.e. set cusum[1] = 0
  cusum <- matrix(0,nrow=(length(control$range)+1), ncol=ncol(x))
  #alarm <- matrix(data = 0, nrow = (length(control$range)+1), ncol = 1)

  for (t in 1:length(control$range)){
    # compute cumulated sums of standardized observations corrected with the
    # reference value k for all time points in range
    cusum[t+1,]<- pmax(0, cusum[t,]+(standObs[t,]-control$k))
  }
  # give alarm if the cusum is larger than the decision boundary h
  alarm <- cusum > control$h

  return(list(cusum=cusum,alarm=alarm,h=control$h,k=control$k,theta0=m))
}


@
